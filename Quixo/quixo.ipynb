{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iplov\\Desktop\\CI 2023\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from game import Game, Move, Player\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift90 = np.array([4,9,14,19,24,3,8,13,18,23,2,7,12,17,22,1,6,11,16,21,0,5,10,15,20])\n",
    "shift180 = shift90[shift90]\n",
    "shift270 = shift180[shift90]\n",
    "mirror = np.array([4,3,2,1,0,9,8,7,6,5,14,13,12,11,10,19,18,17,16,15,24,23,22,21,20]) \n",
    "m_shift90 = mirror[shift90]\n",
    "m_shift180 = mirror[shift180]\n",
    "m_shift270 = mirror[shift270]\n",
    "SYMMETRIES = [shift90,shift180,shift270,m_shift90,m_shift180,m_shift270,mirror]\n",
    "MOVES_SHIFT = [Move.TOP,Move.LEFT,Move.BOTTOM,Move.RIGHT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game class that uses symmetries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningGame(Game):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def set_state(self, state):\n",
    "        board,_ = LearningGame.board_from_state(state)\n",
    "        self._board = board\n",
    "    \n",
    "    def set_board(self, board):\n",
    "        self._board = board\n",
    "    \n",
    "    def state_from_board(self,player: int):\n",
    "        board = self.get_board()\n",
    "        board = board.flatten()\n",
    "        Xs = board == 0\n",
    "        Os = board == 1\n",
    "        key = tuple(map(tuple, (Xs, Os))), player\n",
    "        return key\n",
    "\n",
    "    def board_from_state(state):\n",
    "        board = np.ones(25, dtype=np.uint8) * -1\n",
    "        Xs_Os,player = state\n",
    "        Xs,Os = Xs_Os\n",
    "        board[list(Xs)] = 0\n",
    "        board[list(Os)] = 1\n",
    "        return board.reshape(5,5), player\n",
    "    \n",
    "    def move(self, from_pos: tuple[int, int], slide: Move, player_id: int) -> bool:\n",
    "        '''Perform a move'''\n",
    "        if player_id not in (0, 1):\n",
    "            return False\n",
    "        prev_value = deepcopy(self._board[(from_pos[1], from_pos[0])])\n",
    "        acceptable = self.take((from_pos[1], from_pos[0]), player_id)\n",
    "        if acceptable:\n",
    "            acceptable = self.slide((from_pos[1], from_pos[0]), slide)\n",
    "            if not acceptable:  # restore previous\n",
    "                self._board[(from_pos[1], from_pos[0])] = deepcopy(prev_value)\n",
    "        return acceptable\n",
    "\n",
    "    def take(self, from_pos: tuple[int, int], player_id: int) -> bool:\n",
    "        \"\"\"Checks that {from_pos} is in the border and marks the cell with {player_id}\"\"\"\n",
    "        row, col = from_pos\n",
    "        from_border = row in (0, 4) or col in (0, 4)\n",
    "        if not from_border:\n",
    "            return False  # the cell is not in the border\n",
    "        if self._board[from_pos] != player_id and self._board[from_pos] != -1:\n",
    "            return False  # the cell belongs to the opponent\n",
    "        self._board[from_pos] = player_id\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def acceptable_slides(from_position: tuple[int, int]):\n",
    "        \"\"\"When taking a piece from {from_position} returns the possible moves (slides)\"\"\"\n",
    "        acceptable_slides = [Move.BOTTOM, Move.TOP, Move.LEFT, Move.RIGHT]\n",
    "        axis_0 = from_position[0]    # axis_0 = 0 means uppermost row\n",
    "        axis_1 = from_position[1]    # axis_1 = 0 means leftmost column\n",
    "\n",
    "        if axis_0 == 0:  # can't move upwards if in the top row...\n",
    "            acceptable_slides.remove(Move.TOP)\n",
    "        elif axis_0 == 4:\n",
    "            acceptable_slides.remove(Move.BOTTOM)\n",
    "\n",
    "        if axis_1 == 0:\n",
    "            acceptable_slides.remove(Move.LEFT)\n",
    "        elif axis_1 == 4:\n",
    "            acceptable_slides.remove(Move.RIGHT)\n",
    "        return acceptable_slides\n",
    "\n",
    "    def slide(self, from_pos: tuple[int, int], slide: Move) -> bool:\n",
    "        '''Slide the other pieces'''\n",
    "        if slide not in self.acceptable_slides(from_pos):\n",
    "            return False  # consider raise ValueError('Invalid argument value')\n",
    "        axis_0, axis_1 = from_pos\n",
    "        # np.roll performs a rotation of the element of a 1D ndarray\n",
    "        if slide == Move.RIGHT:\n",
    "            self._board[axis_0] = np.roll(self._board[axis_0], -1)\n",
    "        elif slide == Move.LEFT:\n",
    "            self._board[axis_0] = np.roll(self._board[axis_0], 1)\n",
    "        elif slide == Move.BOTTOM:\n",
    "            self._board[:, axis_1] = np.roll(self._board[:, axis_1], -1)\n",
    "        elif slide == Move.TOP:\n",
    "            self._board[:, axis_1] = np.roll(self._board[:, axis_1], 1)\n",
    "        return True\n",
    "    \n",
    "    def get_available_moves(self,player: int) -> list[tuple[tuple[int, int], Move]]:\n",
    "        available_moves = []\n",
    "        l = [0,1,2,3,4]\n",
    "        l2 = [0,4]\n",
    "        p1 = list(product(l, l2))\n",
    "        p2 = list(product(l2, l))\n",
    "        positions = set(p1) | set(p2)\n",
    "        syms = []\n",
    "        original_board = self.get_board().flatten()\n",
    "        for sym in SYMMETRIES:\n",
    "            board = original_board[sym]\n",
    "            if (original_board == board).all():\n",
    "                syms.append(sym)\n",
    "        \n",
    "        while positions:\n",
    "            row,col = positions.pop()\n",
    "            if self._board[row, col] == -1 or self._board[row, col] == player: \n",
    "                for move in self.acceptable_slides((row,col)):\n",
    "                    available_moves.append(((col, row), move))\n",
    "            else:\n",
    "                continue\n",
    "            index = row * 5 + col\n",
    "            for sym in syms:\n",
    "                idx_pos = sym[index]\n",
    "                pos = idx_pos // 5, idx_pos % 5\n",
    "                positions.discard(pos)\n",
    "        return available_moves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomPlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        from_pos = (random.randint(0, 4), random.randint(0, 4))\n",
    "        move = random.choice([Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT])\n",
    "        return from_pos, move\n",
    "\n",
    "#see winning move\n",
    "class MyPlayer(Player):\n",
    "    def __init__(self,player) -> None:\n",
    "        super().__init__()\n",
    "        self.player = player\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        g = LearningGame()\n",
    "        moves = [Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT]\n",
    "        l = [0,1,2,3,4]\n",
    "        l2 = [0,4]\n",
    "        p1 = list(product(l, l2))\n",
    "        p2 = list(product(l2, l))\n",
    "        positions = list(set(p1) | set(p2))\n",
    "        for pos in positions:\n",
    "            for move in moves:\n",
    "                g.set_board(game.get_board())\n",
    "                ok = g.move(pos,move,self.player)\n",
    "                if not ok:\n",
    "                    continue\n",
    "                if g.check_winner() == self.player:\n",
    "                    return pos,move\n",
    "        from_pos = random.choice(positions)\n",
    "        move = random.choice([Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT])\n",
    "        return from_pos, move\n",
    "\n",
    "class MyPlayer2(Player):\n",
    "    def __init__(self,n: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        from_pos = (self.n, 0)\n",
    "        move = Move.BOTTOM\n",
    "        return from_pos, move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning strategy (Tried but not good -> too many states to save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(Player):\n",
    "    def __init__(self, player = 0,alpha = 0.1, gamma = 0.9, epsilon = 0.1):\n",
    "        # Instead of using a 2D matrix, the q_table is implemented as a dictionary where the keys are tuples representing states, \n",
    "        # and the values are arrays that represent the Q values associated with possible actions in the corresponding state.\n",
    "        self.q_table = defaultdict(lambda: np.zeros((16, 4), dtype=float))\n",
    "        self.alpha = alpha    # Learning rate\n",
    "        self.gamma = gamma    # Discount factor\n",
    "        self.epsilon = epsilon # Exploration rate\n",
    "        self.player = player # Indicate if it is X or O\n",
    "\n",
    "    def translate_action(self,action):\n",
    "        '''\n",
    "        from index (i,j) to an action ((column,row),Move)\n",
    "        '''\n",
    "        pos,m = action[0], action[1]\n",
    "        if pos < 10:\n",
    "            tile = pos%5, pos//5 * 4\n",
    "        else:\n",
    "            tile = pos%2 * 4, pos // 2 - 4\n",
    "        return tile,Move(m)\n",
    "    \n",
    "    def perform_actual_state(self,state):\n",
    "        if state in self.q_table:\n",
    "            return state, -1\n",
    "        state1,player = state\n",
    "        Xs,Os = state1\n",
    "        Xs = np.array(Xs)\n",
    "        Os = np.array(Os)\n",
    "        for i,s in enumerate(SYMMETRIES):\n",
    "            X_shift = Xs[s]\n",
    "            O_shift = Os[s]\n",
    "            actual_state = tuple(map(tuple, (X_shift, O_shift))), player\n",
    "            if actual_state in self.q_table:\n",
    "                return actual_state, i\n",
    "        return state, -1\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            # Explore: choose a random valid action\n",
    "            q_values = self.q_table[state]\n",
    "            indices = np.where(q_values != float(\"-inf\"))\n",
    "            actions = list(zip(indices[0],indices[1]))\n",
    "            selected = random.choice(actions)\n",
    "            return selected\n",
    "        else:\n",
    "            # Exploit: choose the best action based on current Q-values\n",
    "            # If not present the state as a key in the dict, it will be created\n",
    "            q_values = self.q_table[state]\n",
    "            max_values = np.max(q_values)\n",
    "            max_indices = np.where(q_values == max_values)\n",
    "            actions = list(zip(max_indices[0],max_indices[1]))\n",
    "            selected = random.choice(actions)\n",
    "            return selected\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # Represents the current Q value for the \"action\" in the \"state\"\n",
    "        old_value = self.q_table[state][action]\n",
    "\n",
    "        actual_next_state, _ = self.perform_actual_state(next_state)\n",
    "        # Represents the maximum estimated Q value for valid actions in the \"next_state\"\n",
    "        future_q = 0 if reward != 0 else -np.max(self.q_table[actual_next_state])\n",
    "        \n",
    "        # Bellman Equation: Q(s,a)←(1−α)⋅Q(s,a)+α⋅(r+γ⋅maxQ(s′,a′))\n",
    "        # where Q(s,a) is the current Q value for state s and action a\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * future_q)\n",
    "        self.q_table[state][action] = new_value\n",
    "        return actual_next_state\n",
    "\n",
    "    def learn_error(self,state,action):\n",
    "        self.q_table[state][action] = float(\"-inf\")\n",
    "\n",
    "    def save_q_table(self,name):\n",
    "        # Salva il dizionario utilizzando pickle\n",
    "        with open(f\"{name}.pkl\", 'wb') as pickle_file:\n",
    "            pickle.dump(dict(self.q_table), pickle_file)\n",
    "\n",
    "    def load_q_table(self,name):\n",
    "        # Apri il file pickle in modalità lettura binaria (rb)\n",
    "        with open(f\"{name}.pkl\", 'rb') as pickle_file:\n",
    "            # Carica il dizionario da file pickle\n",
    "            loaded_dict = pickle.load(pickle_file)\n",
    "\n",
    "        # Converti il dizionario in una defaultdict\n",
    "        loaded_defaultdict = defaultdict(lambda: np.zeros((16, 4), dtype=float), loaded_dict)\n",
    "        self.q_table = loaded_defaultdict\n",
    "\n",
    "    def set_player(self,player: int):\n",
    "        self.player = player\n",
    "        \n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        state = LearningGame.state_from_board(game,self.player)\n",
    "        actual_state,idx = self.perform_actual_state(state)\n",
    "        q_values = self.q_table[actual_state]\n",
    "        max_values = np.max(q_values)\n",
    "        max_indices = np.where(q_values == max_values)\n",
    "        actions = list(zip(max_indices[0],max_indices[1]))\n",
    "        selected = random.choice(actions)\n",
    "        tile,move = self.translate_action(selected)\n",
    "        if idx < 0:\n",
    "            return tile,move\n",
    "        \n",
    "        i = tile[0] + tile[1] * 5  \n",
    "        pos = np.where(SYMMETRIES[idx] == i)[0][0]    \n",
    "        tile = pos%5, pos//5\n",
    "\n",
    "        if idx>2:\n",
    "            if move == Move.LEFT:\n",
    "                move = Move.RIGHT\n",
    "            elif move == Move.RIGHT:\n",
    "                move = Move.LEFT\n",
    "            idx -= 3\n",
    "        if idx <=2:\n",
    "            idx += 1\n",
    "            index_move = MOVES_SHIFT.index(move)\n",
    "            index_actual_move = (idx + index_move) % 4\n",
    "            move = MOVES_SHIFT[index_actual_move]\n",
    "        \n",
    "        return tile,move\n",
    "    \n",
    "\n",
    "\n",
    "def train_agent(episodes):\n",
    "    agent = QLearningAgent()\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        g = LearningGame()\n",
    "        current_player_idx = 0\n",
    "        state = g.state_from_board(current_player_idx)\n",
    "        winner = -1\n",
    "        while winner < 0:\n",
    "            ok = False\n",
    "            while not ok:\n",
    "                action = agent.choose_action(state)\n",
    "                from_pos, slide = agent.translate_action(action)\n",
    "                ok = g.move(from_pos, slide, current_player_idx)\n",
    "                if not ok:\n",
    "                    agent.learn_error(state,action)\n",
    "            \n",
    "            winner = g.check_winner()\n",
    "            reward = 0\n",
    "            if winner == current_player_idx:\n",
    "                reward = 1\n",
    "            elif winner == (1 - current_player_idx):\n",
    "                reward = -1\n",
    "\n",
    "            current_player_idx = 1 - current_player_idx\n",
    "            next_state = g.state_from_board(current_player_idx)\n",
    "\n",
    "            actual_next_state = agent.learn(state,action,reward,next_state)\n",
    "            g.set_state(actual_next_state)\n",
    "            state = actual_next_state\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "#trained_agent = train_agent(100)\n",
    "\n",
    "trained_agent = QLearningAgent(0)\n",
    "trained_agent.load_q_table(\"q_table100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I evaluated 3586324 states\n"
     ]
    }
   ],
   "source": [
    "#trained_agent.save_q_table(\"q_table100k\")\n",
    "q_table = trained_agent.q_table\n",
    "print(f\"I evaluated {len(q_table)} states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True), (False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False)), 0)\n",
      "Stato con il massimo valore nella Q-table:\n",
      " (array([[-1, -1,  1, -1,  0],\n",
      "       [ 1, -1, -1, -1,  0],\n",
      "       [-1, -1, -1, -1, -1],\n",
      "       [-1, -1, -1, -1,  0],\n",
      "       [ 1, -1,  1, -1,  0]], dtype=int16), 0)\n",
      "Valori associati:\n",
      " [[0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.271]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.     -inf 0.   ]\n",
      " [0.    0.    0.    0.   ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for key, v in q_table.items():\\n    state1,player = key\\n    Xs,Os = state1\\n    Xs = np.array(Xs)\\n    Os = np.array(Os)\\n    for i,s in enumerate(SYMMETRIES):\\n        X_shift = Xs[s]\\n        O_shift = Os[s]\\n        actual_state = tuple(map(tuple, (X_shift, O_shift))), player\\n        board_now = LearningGame.board_from_state(key)\\n        board_s = LearningGame.board_from_state(actual_state)\\n        if np.array_equal(board_s, board_now) and actual_state in q_table:\\n            print(\"-\"*25)\\n            print(\"Stato iniziale:\\n\", LearningGame.board_from_state(key))\\n            print(\"Stato simmetrico:\\n\", LearningGame.board_from_state(actual_state))\\n            print(\"-\"*25)\\n            break'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = trained_agent.q_table\n",
    "max_state = max(q_table, key=lambda state: np.max(q_table[state]))\n",
    "print(max_state)\n",
    "state1,player = max_state\n",
    "Xs,Os = state1\n",
    "Xs = np.array(Xs)\n",
    "Os = np.array(Os)\n",
    "print(\"Stato con il massimo valore nella Q-table:\\n\", LearningGame.board_from_state(max_state))\n",
    "print(\"Valori associati:\\n\", q_table[max_state])\n",
    "\n",
    "'''for key, v in q_table.items():\n",
    "    state1,player = key\n",
    "    Xs,Os = state1\n",
    "    Xs = np.array(Xs)\n",
    "    Os = np.array(Os)\n",
    "    for i,s in enumerate(SYMMETRIES):\n",
    "        X_shift = Xs[s]\n",
    "        O_shift = Os[s]\n",
    "        actual_state = tuple(map(tuple, (X_shift, O_shift))), player\n",
    "        board_now = LearningGame.board_from_state(key)\n",
    "        board_s = LearningGame.board_from_state(actual_state)\n",
    "        if np.array_equal(board_s, board_now) and actual_state in q_table:\n",
    "            print(\"-\"*25)\n",
    "            print(\"Stato iniziale:\\n\", LearningGame.board_from_state(key))\n",
    "            print(\"Stato simmetrico:\\n\", LearningGame.board_from_state(actual_state))\n",
    "            print(\"-\"*25)\n",
    "            break'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniMax Agent (The Choosen one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuixoMinimaxAgent(Player):\n",
    "    def __init__(self, player: int, depth: int):\n",
    "        self.player = player\n",
    "        self.depth = depth\n",
    "        self.board = LearningGame()\n",
    "        self.initial_val = float('inf') if player == 0 else float('-inf')\n",
    "\n",
    "    def make_move(self, quixo_game: Game) -> tuple[tuple[int, int], Move]:\n",
    "        self.board.set_board(quixo_game.get_board())\n",
    "        _, move = self.minimax(deepcopy(self.board), depth=self.depth, maximizing_player = self.player, evaluation_prev_node = self.initial_val)\n",
    "        return move\n",
    "\n",
    "    def minimax(self, quixo_game: LearningGame, depth: int, maximizing_player: int, evaluation_prev_node: float):\n",
    "        if depth == 0 or quixo_game.check_winner() != -1:\n",
    "            return self.evaluate(quixo_game), None\n",
    "\n",
    "        possible_moves = quixo_game.get_available_moves(maximizing_player)\n",
    "\n",
    "        if maximizing_player == 0:\n",
    "            max_eval = float('-inf')\n",
    "            best_move = None\n",
    "            for tile,move in possible_moves:\n",
    "                if max_eval == float('inf'):\n",
    "                    return max_eval, best_move\n",
    "                if max_eval >= evaluation_prev_node:\n",
    "                    return max_eval, best_move\n",
    "                \n",
    "                new_game = deepcopy(quixo_game)\n",
    "                ok = new_game.move(tile,move,0)\n",
    "                if not ok:\n",
    "                    print(tile, move)\n",
    "                    continue\n",
    "                evaluation, _ = self.minimax(new_game, depth - 1, 1, max_eval)\n",
    "                if evaluation > max_eval:\n",
    "                    max_eval = evaluation\n",
    "                    best_move = tile,move\n",
    "            return max_eval, best_move\n",
    "        else:\n",
    "            min_eval = float('inf')\n",
    "            best_move = None\n",
    "            for tile,move in possible_moves:\n",
    "                if min_eval == float('-inf'):\n",
    "                    return min_eval, best_move\n",
    "                if min_eval <= evaluation_prev_node:\n",
    "                    return min_eval, best_move\n",
    "                \n",
    "                new_game = deepcopy(quixo_game)\n",
    "                ok = new_game.move(tile,move,1)\n",
    "                if not ok:\n",
    "                    print(tile, move)\n",
    "                    continue\n",
    "                evaluation, _ = self.minimax(new_game, depth - 1, 0, min_eval)\n",
    "                if evaluation < min_eval:\n",
    "                    min_eval = evaluation\n",
    "                    best_move = tile,move\n",
    "            return min_eval, best_move\n",
    "\n",
    "    def evaluate(self, quixo_game: LearningGame) -> float:\n",
    "        winner = quixo_game.check_winner()\n",
    "        if winner == 1:\n",
    "            return float('-inf')\n",
    "        elif winner == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        X = []\n",
    "        O = []\n",
    "        for i in range(quixo_game._board.shape[0]):\n",
    "            x_r = 0\n",
    "            x_c = 0\n",
    "            o_r = 0\n",
    "            o_c = 0\n",
    "            for j in range(quixo_game._board.shape[1]):\n",
    "                if quixo_game._board[i,j] == 0:\n",
    "                    x_r += 1\n",
    "                elif quixo_game._board[i,j] == 1:\n",
    "                    o_r += 1\n",
    "                \n",
    "                if quixo_game._board[j,i] == 0:\n",
    "                    x_c += 1\n",
    "                elif quixo_game._board[j,i] == 1:\n",
    "                    o_c += 1\n",
    "            X.append(x_r)\n",
    "            X.append(x_c)\n",
    "            O.append(o_r)\n",
    "            O.append(o_c)\n",
    "\n",
    "        x_d1 = 0\n",
    "        x_d2 = 0\n",
    "        o_d1 = 0\n",
    "        o_d2 = 0\n",
    "        for i in range(quixo_game._board.shape[0]):\n",
    "            if quixo_game._board[i,i] == 0:\n",
    "                x_d1 += 1\n",
    "            elif quixo_game._board[i,i] == 1:\n",
    "                o_d1 += 1\n",
    "\n",
    "            if quixo_game._board[i, -(i+1)] == 0:\n",
    "                x_d2 += 1\n",
    "            elif quixo_game._board[i,-(i+1)] == 1:\n",
    "                o_d2 += 1\n",
    "\n",
    "        evaluation = 0\n",
    "        for i in range(len(X)):\n",
    "            val = X[i] / 4 - O[i] / 4\n",
    "            if X[i] > O[i]:\n",
    "                val += X[i]*0.1\n",
    "            \n",
    "            if X[i] < O[i]:\n",
    "                val -= O[i]*0.1\n",
    "\n",
    "            if i<2 or i>=len(X)-2:\n",
    "                val = val * 1.5\n",
    "            evaluation += val\n",
    "        \n",
    "        val = x_d1 / 4 - o_d1 / 4\n",
    "        if x_d1 > o_d1:\n",
    "            val += x_d1*0.1\n",
    "        \n",
    "        if x_d1 < o_d1:\n",
    "            val -= o_d1*0.1\n",
    "        \n",
    "        evaluation += val\n",
    "\n",
    "        val = x_d2 / 4 - o_d2 / 4\n",
    "        if x_d2 > o_d2:\n",
    "            val += x_d2*0.1\n",
    "        \n",
    "        if x_d2 < o_d2:\n",
    "            val -= o_d2*0.1\n",
    "        evaluation += val\n",
    "        return evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [43:33<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent wins 1000 game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_wins = 0\n",
    "for i in tqdm(range(1000)):\n",
    "    g = LearningGame()\n",
    "    player1 = QuixoMinimaxAgent(0,2)\n",
    "    player2 = MyPlayer(1)\n",
    "    winner = g.play(player1, player2)\n",
    "    if winner == 0:\n",
    "        n_wins += 1\n",
    "print(f\"Agent wins {n_wins} game\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [52:35<00:00,  3.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent wins 1000 game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_wins = 0\n",
    "for i in tqdm(range(1000)):\n",
    "    g = LearningGame()\n",
    "    player1 = MyPlayer(0)\n",
    "    player2 = QuixoMinimaxAgent(1,2)\n",
    "    winner = g.play(player1, player2)\n",
    "    if winner == 1:\n",
    "        n_wins += 1\n",
    "print(f\"Agent wins {n_wins} game\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
